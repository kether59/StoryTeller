# ü§ñ Configuration de l'Assistant d'√âcriture LLM

## Vue d'ensemble

L'assistant d'√©criture utilise des LLM (Large Language Models) pour vous aider √† :
- ‚úçÔ∏è G√©n√©rer des chapitres complets
- ‚û°Ô∏è Continuer l'√©criture de manuscrits existants
- üîÑ R√©√©crire et am√©liorer des textes
- üí° Sugg√©rer des id√©es de sc√®nes

## Providers support√©s

### 1. Anthropic Claude (Recommand√©) ‚≠ê

Claude est excellent pour l'√©criture cr√©ative et respecte bien les contextes longs.

#### Installation

```bash
pip install anthropic
```

#### Configuration

Cr√©ez un fichier `.env` dans le dossier `backend/` :

```env
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxx
```

**Obtenir une cl√© API :**
1. Allez sur https://console.anthropic.com/
2. Cr√©ez un compte
3. Allez dans "API Keys"
4. Cr√©ez une nouvelle cl√©
5. Copiez-la dans votre `.env`

**Prix :** ~$3-8 pour 1 million de tokens d'entr√©e, ~$15-24 pour 1 million de tokens de sortie

---

### 2. OpenAI GPT-4

Alternative populaire avec GPT-4.

#### Installation

```bash
pip install openai
```

#### Configuration

```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxxxxxxxxxxx
```

**Obtenir une cl√© API :**
1. Allez sur https://platform.openai.com/
2. Cr√©ez un compte
3. Ajoutez un moyen de paiement
4. Cr√©ez une cl√© API
5. Copiez-la dans votre `.env`

**Prix :** Varie selon le mod√®le (GPT-4 Turbo ~$10/1M tokens entr√©e, $30/1M tokens sortie)

---

### 3. Ollama (Gratuit, Local) üÜì

Pour ex√©cuter des mod√®les localement sans co√ªt ni limite.

#### Installation

**Sur Windows :**
1. T√©l√©chargez Ollama : https://ollama.com/download
2. Installez et lancez l'application

**Sur Linux/Mac :**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### T√©l√©charger un mod√®le

```bash
# Mod√®les fran√ßais recommand√©s
ollama pull mistral              # 7B - Rapide et bon
ollama pull mixtral              # 8x7B - Plus puissant
ollama pull llama3               # Alternative
```

#### Configuration

```env
LLM_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
```

**Avantages :**
- ‚úÖ Gratuit
- ‚úÖ Donn√©es priv√©es (tout est local)
- ‚úÖ Pas de limite d'utilisation

**Inconv√©nients :**
- ‚ö†Ô∏è N√©cessite un GPU pour √™tre rapide
- ‚ö†Ô∏è Qualit√© l√©g√®rement inf√©rieure aux mod√®les cloud

---

## üì¶ Installation des d√©pendances

Mettez √† jour votre `requirements.txt` :

```txt
# LLM Providers (choisir selon vos besoins)
anthropic>=0.18.0  # Pour Claude
openai>=1.10.0     # Pour OpenAI
httpx>=0.26.0      # Pour Ollama

# Existant
fastapi>=0.104.1
uvicorn[standard]>=0.24.0
sqlalchemy>=2.0.23
pydantic>=2.5.0
pydantic-settings>=2.1.0
spacy>=3.7.0
```

Puis installez :

```bash
pip install -r requirements.txt
```

---

## üöÄ Lancement

### 1. Avec variables d'environnement

```bash
# Dans backend/
export ANTHROPIC_API_KEY="sk-ant-api03-xxxxx"
export LLM_PROVIDER="anthropic"

uvicorn main:app --reload
```

### 2. Avec fichier .env

Cr√©ez `backend/.env` :

```env
LLM_PROVIDER=anthropic
ANTHROPIC_API_KEY=sk-ant-api03-xxxxxxxxxxxxx

# Ou pour OpenAI
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxxxxxxx

# Ou pour Ollama
# LLM_PROVIDER=ollama
# OLLAMA_URL=http://localhost:11434
```

Puis lancez normalement :

```bash
uvicorn main:app --reload
```

---

## üß™ Test de configuration

### 1. Via l'API

```bash
curl http://localhost:8000/api/llm/health
```

R√©ponse attendue :
```json
{
  "provider": "anthropic",
  "configured": true
}
```

### 2. Via l'interface

1. Ouvrez http://localhost:5173
2. Allez dans l'onglet "‚úçÔ∏è Assistant d'√©criture"
3. Vous devriez voir : "ü§ñ LLM: anthropic ‚úÖ Configur√©"

---

## üí° Utilisation

### G√©n√©rer un chapitre complet

1. Onglet "‚úçÔ∏è Assistant d'√©criture"
2. Mode "üìù G√©n√©rer un chapitre"
3. Remplissez :
    - R√©sum√© de ce qui doit se passer
    - Style, longueur, ton
    - S√©lectionnez personnages et lieux
4. Cliquez sur "‚ú® G√©n√©rer le chapitre"
5. Attendez 20-60 secondes
6. Sauvegardez le r√©sultat ou copiez-le

### Continuer l'√©criture

1. Mode "‚û°Ô∏è Continuer l'√©criture"
2. S√©lectionnez un manuscrit existant
3. Indiquez la direction souhait√©e
4. Cliquez sur "‚û°Ô∏è Continuer l'√©criture"

### R√©√©crire un texte

1. Mode "üîÑ R√©√©crire un texte"
2. Collez votre texte
3. Donnez des instructions (ex: "Rendre plus descriptif")
4. Comparez l'original et la version r√©√©crite

### Suggestions de sc√®nes

1. Mode "üí° Sugg√©rer une sc√®ne"
2. D√©crivez la situation actuelle
3. Obtenez 5 id√©es de sc√®nes possibles

---

## üí∞ Gestion des co√ªts

### Anthropic Claude

**Estimation pour un roman :**
- G√©n√©ration de 10 chapitres (15000 mots chacun) : ~$5-15
- Le contexte (personnages, lieux, lore) est envoy√© √† chaque fois

**Conseils :**
- Limitez le nombre de personnages/lieux envoy√©s dans le contexte
- Utilisez la longueur "court" pour tester
- Surveillez votre usage sur https://console.anthropic.com/

### OpenAI

**Estimation similaire :** $10-20 pour 10 chapitres avec GPT-4

### Ollama

**Gratuit !** Mais n√©cessite :
- 8-16 GB de RAM
- GPU recommand√© (sinon tr√®s lent)
- 4-7 GB d'espace disque par mod√®le

---

## üîß Personnalisation

### Modifier les prompts

√âditez `backend/routes/llm.py` :

```python
def build_system_prompt(context: dict) -> str:
    # Personnalisez ici le prompt syst√®me
    prompt = f"""Tu es un assistant d'√©criture...
    
    [Ajoutez vos instructions personnalis√©es]
    """
    return prompt
```

### Ajuster les longueurs

```python
# Dans la route /generate-chapter
length_guide = {
    "court": "500-800 mots",      # Modifiez ici
    "moyen": "1000-1500 mots",
    "long": "2000-3000 mots"
}
```

### Changer de mod√®le

**Claude :**
```python
model="claude-sonnet-4-20250514"  # Plus intelligent
model="claude-opus-4-20250514"    # Le meilleur mais plus cher
```

**OpenAI :**
```python
model="gpt-4-turbo-preview"   # Plus rapide
model="gpt-4"                 # Plus classique
```

**Ollama :**
```env
# Dans .env ou au lancement
OLLAMA_MODEL=mistral
# ou
OLLAMA_MODEL=mixtral
```

---

## üêõ D√©pannage

### "LLM provider 'anthropic' non support√©"

‚Üí V√©rifiez que `LLM_PROVIDER` est bien d√©fini dans `.env`

### "ANTHROPIC_API_KEY non configur√©e"

‚Üí Ajoutez votre cl√© API dans `.env`

### "Module 'anthropic' non install√©"

```bash
pip install anthropic
```

### G√©n√©ration tr√®s lente

- **Ollama :** Normal sans GPU. Essayez un mod√®le plus petit (`mistral` au lieu de `mixtral`)
- **Cloud :** V√©rifiez votre connexion internet

### Erreur "Rate limit exceeded"

‚Üí Vous avez d√©pass√© les limites de votre compte. Attendez ou ajoutez des cr√©dits.

### Le LLM ne respecte pas le contexte

‚Üí Votre contexte est peut-√™tre trop long. Limitez le nombre de personnages/lieux envoy√©s.

---

## üìö Ressources

- **Claude :** https://docs.anthropic.com/
- **OpenAI :** https://platform.openai.com/docs
- **Ollama :** https://ollama.com/library

---

## ‚öñÔ∏è Consid√©rations l√©gales

- Les textes g√©n√©r√©s par IA peuvent √™tre prot√©g√©s par le droit d'auteur selon votre juridiction
- V√©rifiez toujours et √©ditez les textes g√©n√©r√©s
- L'IA est un **assistant**, pas un **rempla√ßant** de votre cr√©ativit√©

---

**Bon usage de l'assistant d'√©criture ! ‚úçÔ∏è‚ú®**